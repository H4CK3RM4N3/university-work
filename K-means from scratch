# In[1]:


import numpy as np
import os
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns



# In[2]:


def heatmap(confsuion_mat, clusters_num):
    yticklabels = []
    for k in range(clusters_num):
        yticklabels.append('Cluster '+str(k+1)) 
    fig, ax = plt.subplots(figsize=(10,5))
    sns.heatmap(confsuion_mat, annot=True,fmt='g', ax = ax);
    ax.set_xlabel('Classes\n');
    ax.xaxis.set_label_position('top') 
    ax.set_ylabel('Clusters\n'); 
    ax.set_title('\n ' +str(clusters_num)+' Clusters \n', fontsize=16); 
    ax.xaxis.set_ticklabels(['animals', 'countries', 'fruits', 'veggies']); 
    ax.xaxis.tick_top()
    ax.yaxis.set_ticklabels(yticklabels, rotation=90);
    plt.yticks(rotation=0) 


def bar_graph (evaluations2, title = 'Evaluations'):
    index = []
    for i in range(len(evaluations2)):
        index.append('k = '+str(i+1)) 
    df1 = pd.DataFrame(evaluations2, columns =["Precision", "Recall", "F_score"], index = index)
    df1.head()
    ax = df1.plot.bar(figsize=(18,4));
    plt.legend(bbox_to_anchor=(1.0, 1.0));
    plt.title(title)
    for p in ax.patches:
        ax.annotate(str(np.round(p.get_height(),3)), (p.get_x() * 1.005, p.get_height() * 1.005))
        


# In[3]:


path = 'clustering-data//'


# ## Data reading

# In[4]:


myrow = []
for list1 in os.listdir(path): 
    with open(path + list1) as f:
        rows = [line.rstrip('\n').split(' ') for line in f]
    myrow.append(rows)


# In[5]:


df = pd.DataFrame(np.concatenate([myrow[0], myrow[1], myrow[2], myrow[3]], axis = 0))
classes = np.concatenate([np.zeros((len(myrow[0]))),np.zeros((len(myrow[1]))) + 1,
                          np.zeros((len(myrow[2]))) + 2, np.zeros((len(myrow[3]))) + 3])
df['classes'] = classes
df.head()


# In[6]:


data = df.iloc[:,1:].values
X = np.float_(data[:,:-1])
y = data[:,-1]
X.shape


# ## Q1

# In[7]:


class myKMeans:
    def __init__(self,k):
        self.k = k

    def train(self,X, MAXITER = 100, TOL = 1e-4):
        np.random.seed(0)
        centroids = np.random.rand(self.k,X.shape[1])
        centroidsold = centroids.copy()
        #print(centroidsold.shape)
        for iter_ in range(MAXITER):
            dist = np.linalg.norm(X - centroids[0,:],axis=1).reshape(-1,1)
            #print(dist.shape)
            for class_ in range(1,self.k):
                dist = np.append(dist,np.linalg.norm(X - centroids[class_,:],axis=1).reshape(-1,1),axis=1)
            classes = np.argmin(dist,axis=1)
            # update position
            for class_ in set(classes):
                centroids[class_,:] = np.mean(X[classes == class_,:],axis=0)
                
            #print(np.linalg.norm(centroids - centroidsold))
            if np.linalg.norm(centroids - centroidsold) < TOL:
                print(np.linalg.norm(centroids - centroidsold))
                break
                #print('Centroid converged')
        #print(centroids)
        self.centroids = centroids
    
    def predict(self,X):
        dist = np.linalg.norm(X - self.centroids[0,:],axis=1).reshape(-1,1)
        for class_ in range(1,self.k):
            dist = np.append(dist,np.linalg.norm(X - self.centroids[class_,:],axis=1).reshape(-1,1),axis=1)
        classes = np.argmin(dist,axis=1)
        return classes


# ## Q2
# 

# In[8]:


class myKMedians:
    def __init__(self,k):
        self.k = k

    def train(self,X, MAXITER = 100, TOL = 1e-4):
        np.random.seed(0)
        centroids = np.random.rand(self.k,X.shape[1])
        centroidsold = centroids.copy()
        #print(centroidsold.shape)
        for iter_ in range(MAXITER):
            dist = np.linalg.norm(X - centroids[0,:],axis=1, ord=1).reshape(-1,1)
            #print(dist.shape)
            for class_ in range(1,self.k):
                dist = np.append(dist,np.linalg.norm(X - centroids[class_,:],axis=1, ord=1).reshape(-1,1),axis=1)
            classes = np.argmin(dist,axis=1)
            # update position
            for class_ in set(classes):
                centroids[class_,:] = np.median(X[classes == class_,:],axis=0)
                
            #print(np.linalg.norm(centroids - centroidsold))
            if np.linalg.norm(centroids - centroidsold, ord=1) < TOL:
                print(np.linalg.norm(centroids - centroidsold, ord=1))
                break
                #print('Centroid converged')
        #print(centroids)
        self.centroids = centroids
    
    def predict(self,X):
        dist = np.linalg.norm(X - self.centroids[0,:],axis=1, ord=1).reshape(-1,1)
        for class_ in range(1,self.k):
            dist = np.append(dist,np.linalg.norm(X - self.centroids[class_,:],axis=1, ord=1).reshape(-1,1),axis=1)
        classes = np.argmin(dist,axis=1)
        return classes


# ## Q3
# 

# In[9]:


evaluations3 = []

for clusters_num in range(1,10):

    kmeans = myKMeans(clusters_num)
    kmeans.train(X)
    pred_out = kmeans.predict(X)

    confsuion_mat = np.zeros((clusters_num, 4))
    for k in range(len(pred_out)):
        confsuion_mat[pred_out[k], int(y[k])] +=1

    heatmap(confsuion_mat, clusters_num )
    plt.pause(0.5)

    Precision = np.sum(np.max(confsuion_mat, axis = 1))/np.sum(confsuion_mat)
    Recall = np.sum(np.max(confsuion_mat, axis = 0))/np.sum(confsuion_mat)
    F_score = 2* (Precision*Recall)/ (Precision+Recall)
    
    evaluations3.append([Precision, Recall, F_score])


# In[10]:


bar_graph (evaluations3, title = 'Evaluations Q3')


# ## Q4
# 

# In[22]:


# Normalizing
feature_norms = np.linalg.norm(X, axis = 1)
X_normalized = X/feature_norms[:,None]


# In[23]:


evaluations4 = []

for clusters_num in range(1,10):

    kmeans = myKMeans(clusters_num)
    kmeans.train(X_normalized)
    pred_out = kmeans.predict(X_normalized)

    confsuion_mat = np.zeros((clusters_num, 4))
    for k in range(len(pred_out)):
        confsuion_mat[pred_out[k], int(y[k])] +=1

    heatmap(confsuion_mat, clusters_num )
    plt.pause(0.5)

    Precision = np.sum(np.max(confsuion_mat, axis = 1))/np.sum(confsuion_mat)
    Recall = np.sum(np.max(confsuion_mat, axis = 0))/np.sum(confsuion_mat)
    F_score = 2* (Precision*Recall)/ (Precision+Recall)
    
    evaluations4.append([Precision, Recall, F_score])


# In[13]:


bar_graph (evaluations4, title = 'Evaluations Q4')


# ## Q5

# In[15]:


evaluations5 = []

for clusters_num in range(1,10):

    kmedians = myKMedians(clusters_num)
    kmedians.train(X)
    pred_out = kmedians.predict(X)

    confsuion_mat = np.zeros((clusters_num, 4))
    for k in range(len(pred_out)):
        confsuion_mat[pred_out[k], int(y[k])] +=1

    heatmap(confsuion_mat, clusters_num )
    plt.pause(0.5)

    Precision = np.sum(np.max(confsuion_mat, axis = 1))/np.sum(confsuion_mat)
    Recall = np.sum(np.max(confsuion_mat, axis = 0))/np.sum(confsuion_mat)
    F_score = 2* (Precision*Recall)/ (Precision+Recall)
    
    evaluations5.append([Precision, Recall, F_score])


# In[17]:


bar_graph (evaluations5, title = 'Evaluations Q5')


# ## Q6

# In[18]:


# Normalizing


# In[19]:


evaluations6 = []

for clusters_num in range(1,10):

    kmedians = myKMedians(clusters_num)
    kmedians.train(X_normalized)
    pred_out = kmedians.predict(X_normalized)

    confsuion_mat = np.zeros((clusters_num, 4))
    for k in range(len(pred_out)):
        confsuion_mat[pred_out[k], int(y[k])] +=1

    heatmap(confsuion_mat, clusters_num )
    plt.pause(0.5)

    Precision = np.sum(np.max(confsuion_mat, axis = 1))/np.sum(confsuion_mat)
    Recall = np.sum(np.max(confsuion_mat, axis = 0))/np.sum(confsuion_mat)
    F_score = 2* (Precision*Recall)/ (Precision+Recall)
    
    evaluations6.append([Precision, Recall, F_score])


# In[20]:


bar_graph (evaluations6, title = 'Evaluations Q6')


# ## Q7
# ### Comparing the different clusterings

# #### **** Results show that in Q2 and k=4, we have better perfromance.

# In[21]:


bar_graph (evaluations3, title = 'Evaluations Q3 (Unnormalised Kmeans)')
bar_graph (evaluations4, title = 'Evaluations Q4 (Normalised Kmeans)')
bar_graph (evaluations5, title = 'Evaluations Q5 (Unnormalised Kmedians)')
bar_graph (evaluations6, title = 'Evaluations Q6 (Normalised Kmedians)')
